{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded (79) images from .\\data\\04042022_data\n"
     ]
    }
   ],
   "source": [
    "import src.helper_func as hf\n",
    "import numpy as np\n",
    "import copy, cv2, math, os, sys\n",
    "import pylab as pl\n",
    "\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score, recall_score\n",
    "\n",
    "# Data load\n",
    "data_07022022 = os.path.join('.', 'data', '07022022_data') # First dataset\n",
    "data_04042022 = os.path.join('.', 'data', '04042022_data') # Second dataset\n",
    "\n",
    "# images, labels = hf.load_tifdata(data_07022022, classes=2, label=True)\n",
    "images, labels = hf.load_tifdata(data_04042022, classes=2, label=True, width=768, height=768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Template Projection and Matching Method for Training-Free Analysis of Delayered IC Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_filters(filter_path, count, shape):\n",
    "    \"\"\"\n",
    "    0. Filters load \n",
    "    parameters \\n\n",
    "        filter_path -> file path for templates \\n\n",
    "        count -> count of templates to load \\n\n",
    "        shape -> shape of templates \\n\n",
    "    return \\n\n",
    "        templates \\n\n",
    "    \"\"\"\n",
    "    filters = np.loadtxt(filter_path).reshape(count, shape[0], shape[1])\n",
    "\n",
    "    for i in range(1,len(filters) + 1): # Filters showing\n",
    "        pl.subplot(4, 4, i)\n",
    "        pl.imshow(filters[i-1], cmap=\"gray\")\n",
    "    pl.show()\n",
    "    return filters\n",
    "\n",
    "def template_expansion(filters, V, H):\n",
    "    \"\"\"\n",
    "    1. Template expansion\n",
    "    parameters \\n\n",
    "        filters -> template to expand \\n\n",
    "        V, H -> number of blocks \n",
    "    return \\n\n",
    "        reshape templates \\n\n",
    "    \"\"\"\n",
    "    templates = []\n",
    "    for filter in filters:\n",
    "        template = np.copy(filter)\n",
    "        for lenght in range(V - 1):\n",
    "            template = np.concatenate((template, filter), axis=0)\n",
    "        filter_image = np.copy(template)\n",
    "        for lenght in range(H - 1):\n",
    "            template = np.concatenate((template, filter_image), axis=1)\n",
    "        templates.append(template)\n",
    "    return templates\n",
    "\n",
    "def image_padding(image, N, M, n, m):\n",
    "    \"\"\"\n",
    "    Add padding of image\n",
    "    parameters \\n\n",
    "        image -> image \\n\n",
    "        N, M -> size of result image \\n\n",
    "        n, m -> size of image \\n\n",
    "    return \\n\n",
    "        padded image \\n\n",
    "    \"\"\"\n",
    "    image['img'] = np.pad(image['img'], (int((N - n) / 2),int((M - m) / 2)), 'constant', constant_values=(0))\n",
    "    image['w'] = image['img'].shape[0]\n",
    "    image['h'] = image['img'].shape[1]\n",
    "    return image\n",
    "\n",
    "\n",
    "def getM(X, Y, h):\n",
    "    M = np.zeros((X, Y, h))\n",
    "    for i in range(X):\n",
    "        for j in range(Y):\n",
    "            if i == j:\n",
    "                M[i][j] = np.ones((h), dtype=int)\n",
    "    return M\n",
    "\n",
    "def global_matching(image, templates, N, H, w, V, M, h):\n",
    "    \"\"\"\n",
    "    2. Global Matching\n",
    "    parameters \\n\n",
    "        image -> image \\n\n",
    "        templates -> templates \\n\n",
    "    return \\n\n",
    "        score maps for image and templates \\n\n",
    "    \"\"\"\n",
    "    S = [] # score map\n",
    "    M_r = getM(H, int(np.ceil(N/w)), w)\n",
    "    M_r = M_r.reshape((H, N)).T\n",
    "\n",
    "    M_l = getM(V, int(np.ceil(M/h)), h)\n",
    "    M_l = M_l.reshape((V, M))\n",
    "\n",
    "    for template in templates:\n",
    "        s = np.sqrt((image - template) ** 2) # L2 distance\n",
    "        S.append(np.dot(np.dot(M_l, s), M_r)) # local summation\n",
    "    return S\n",
    "\n",
    "def template_projection(templates, h, w): \n",
    "    \"\"\"\n",
    "    3. Template Projection\n",
    "    parameters \\n\n",
    "        templates -> templates for projection \\n\n",
    "    return \\n\n",
    "        projected templates \\n\n",
    "    \"\"\"\n",
    "    T_xy = []\n",
    "    for template in templates:\n",
    "        for y in range(h):\n",
    "            for x in range(w):\n",
    "                template_xy = np.roll(template, -x, axis=0)\n",
    "                template_xy = np.roll(template_xy, -y, axis=1)\n",
    "                T_xy.append(template_xy)\n",
    "    return T_xy\n",
    "\n",
    "def global_template_projection_matching(image, templates, N, H, w, V, M, h):\n",
    "    \"\"\"\n",
    "    4. Global Template Projection and Matching\n",
    "    parameters \\n\n",
    "        image -> image to matching \\n \n",
    "        templates -> projected templates  \\n\n",
    "    return \\n\n",
    "        projected score maps \\n\n",
    "    \"\"\"\n",
    "    S_xy = []\n",
    "    M_r = getM(H, int(np.ceil(N/w)), w)\n",
    "    M_r = M_r.reshape((H, N)).T\n",
    "\n",
    "    M_l = getM(V, int(np.ceil(M/h)), h)\n",
    "    M_l = M_l.reshape((V, M))\n",
    "    \n",
    "    for index, template in enumerate(templates):\n",
    "        s = np.sqrt((image - template) ** 2) # L2 distance\n",
    "\n",
    "        x = index % w\n",
    "        y = index // w\n",
    "        S_xy.append(\n",
    "            np.dot(np.dot(np.roll(M_l, -y, axis=1), s), (np.roll(M_r, -x, axis=0)))\n",
    "            ) # local summation\n",
    "    return S_xy\n",
    "\n",
    "\n",
    "def gtpm(shape, filter_path=os.path.join(\".\", \"src\", \"filters_from_data.txt\"), filter_shape=[3, [12, 12]], verbose=True):\n",
    "    \"\"\"\n",
    "    Initialize GTPM, load templates and reshape them \\n\n",
    "    parameters \\n\n",
    "        shape -> shape of image \\n\n",
    "        filter_path -> path for templates \\n\n",
    "        filter_shape -> templates shape \\n\n",
    "    return \\n\n",
    "        expanded templates \\n\n",
    "    \"\"\"\n",
    "    if verbose: print(f\"[+][0] Filter load\")\n",
    "    filters = load_filters(filter_path, filter_shape[0], filter_shape[1])\n",
    "\n",
    "    m = shape[0]  # img_height\n",
    "    n = shape[1]  # img_width\n",
    "    if verbose: print(\"\\timg_height {} img_width {}\".format(m, n))\n",
    "\n",
    "    h = filters[0].shape[0]  # flt_height\n",
    "    w = filters[0].shape[1]  # flt_width\n",
    "    if verbose: print(\"\\tflt_height {} flt_width {}\".format(h, w))\n",
    "\n",
    "    V = int(np.ceil(m / h)) # block height\n",
    "    H = int(np.ceil(n / w)) # block width\n",
    "    if verbose: print(\"\\tV(block height) {} H(block width) {}\".format(V, H))\n",
    "\n",
    "    M = h * V  # tmp_height\n",
    "    N = w * H  # tmp_width\n",
    "    if verbose: print(\"\\ttmp_height {} tmp_width {}\".format(M, N))\n",
    "\n",
    "    if verbose: print(\"[+][1] Template projection\")\n",
    "    expanded_templates = template_projection(template_expansion(filters, V, H), h, w)\n",
    "\n",
    "    return expanded_templates, N, H, w, V, M, h, n, m\n",
    "\n",
    "def gtpm_prediction(image, expanded_templates, N, H, w, V, M, h, n, m):\n",
    "    \"\"\"\n",
    "    Prediction for GTPM\n",
    "    parameters \\n\n",
    "        image -> image \\n\n",
    "        expanded_templates -> expanded templates \\n\n",
    "    return \\n\n",
    "        aggregated score map \\n\n",
    "    \"\"\"\n",
    "    # Copy image and add padding\n",
    "    image = copy.deepcopy(image)\n",
    "    image = image_padding(image, N, M, n, m)\n",
    "\n",
    "    # Compute Score maps (compute distance between template and input data)\n",
    "    score_maps_xy = global_template_projection_matching(image['img'], expanded_templates, N, H, w, V, M, h)\n",
    "\n",
    "    # Aggregate into one score maps\n",
    "    result_map = np.zeros((V, H))\n",
    "    for x in range(V):\n",
    "        for y in range(H):\n",
    "            min = np.iinfo(np.int32).max\n",
    "            index_min = -1\n",
    "            for index, score_map in enumerate(score_maps_xy):\n",
    "                if min > score_map[x, y]:\n",
    "                    min = score_map[x, y]\n",
    "                    index_min = index\n",
    "            result_map[x, y] = index_min \n",
    "    \n",
    "    return result_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matho\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6075949367088608, 0.0, 0.0, 0.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "true_values = []\n",
    "\n",
    "# Prepare GTPM \n",
    "expanded_templates, N, H, w, V, M, h, n, m = gtpm(images[0]['img'].shape, verbose=False)\n",
    "# Prediction on every image\n",
    "for image in images:\n",
    "    # Prediction\n",
    "    result = gtpm_prediction(image, expanded_templates, N, H, w, V, M, h, n, m)\n",
    "\n",
    "    # A classification of which template is the most occur in the result score map\n",
    "    horizontal_metal = np.sum(np.where(result <= 144, 1, 0))\n",
    "    vias = np.sum(np.where((result > 144) & (result < 288), 1, 0))\n",
    "    vertical_metal = np.sum(np.where(result >= 288, 1, 0))\n",
    "\n",
    "    true_values.append(1 if image['Y'] > 0 else 0)\n",
    "    predictions.append(1 if horizontal_metal > vias or vertical_metal > vias else 0)\n",
    "\n",
    "# Print result of classification, print metrics of classification\n",
    "print(predictions)\n",
    "accuracy_score(true_values, predictions), f1_score(true_values, predictions), precision_score(true_values, predictions), recall_score(true_values, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thresholding approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Border for classification\n",
    "border = 120 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  New Iterative Triclass Thresholding Technique in Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import threshold_otsu, threshold_niblack, threshold_sauvola, threshold_triangle\n",
    "import numpy as np\n",
    "import copy\n",
    "import cv2\n",
    "\n",
    "\n",
    "def triclass_threshold(image_dict, threshold_algorithm, preset_thresholding=3, F=None, B=None):\n",
    "    \"\"\"\n",
    "    Iterative triclass thresholding technique\n",
    "    parameters \\n\n",
    "        image_dict -> dict of image \\n\n",
    "        threshold_algorithm -> algorithm for thresholding (func) \\n\n",
    "        preset_thresholding -> preset thresholding to stop iterating \\n\n",
    "        F, B -> foreground and background pixel \\n\n",
    "    return \\n\n",
    "        segmentated image\n",
    "    \"\"\"\n",
    "    result_image = copy.deepcopy(image_dict['img'])\n",
    "    img = copy.deepcopy(image_dict['img'])\n",
    "\n",
    "    thresholds = []\n",
    "    index = 0\n",
    "    while (1):\n",
    "        # At the first iteration, Otsu’s method is applied to find a threshold T^[1]\n",
    "        threshold = threshold_algorithm(img)\n",
    "        thresholds.append(threshold)\n",
    "\n",
    "        # We then find and denote the means of the two classes separated by T^[1]\n",
    "        # as μ[1] 0 and μ[1] 1 for the background and foreground, respectively.\n",
    "        first_class = img[img < int(threshold)]\n",
    "        second_class = img[img > int(threshold)]\n",
    "        \n",
    "        # The iteration stops when the difference between two consecutive threshold \n",
    "        # |T^[n+1] − T^[n]| is less than a preset threshold.\n",
    "        if index > 0 and ((abs(thresholds[index-1] - thresholds[index-2]) < abs(thresholds[index] - thresholds[index-1])) or (abs(thresholds[index-1] - thresholds[index-2]) <= preset_thresholding)):\n",
    "            B = np.concatenate((B, img[img < int(threshold)]), axis=None)\n",
    "            F = np.concatenate((F, img[img > int(threshold)]), axis=None)\n",
    "            result_image = np.where(np.isin(result_image, img[img > int(threshold)]), 1, result_image)\n",
    "            result_image = np.where(result_image == 1, 1, 0)\n",
    "            break\n",
    "\n",
    "        mi_0 = np.mean(first_class)\n",
    "        mi_1 = np.mean(second_class)\n",
    "\n",
    "        # Then we classify regions whose pixel values are greater than μ_1 as foreground F^[1] \n",
    "        # and regions whose pixel values are less than μ_0 as background B^[1]\n",
    "        B = np.concatenate((B, first_class[first_class < mi_0]), axis=None)\n",
    "        F = np.concatenate((F, second_class[second_class > mi_1]), axis=None)\n",
    "        result_image = np.where(np.isin(result_image, second_class[second_class > mi_1]), 1, result_image)\n",
    "\n",
    "        TBD = img[(mi_0 <= img) & (img <= mi_1)] # omega\n",
    "\n",
    "        # At the second iteration, we apply Otsu’s method to find threshold T^[2]\n",
    "        # on region omega[1] only.\n",
    "        img = TBD\n",
    "        index += 1\n",
    "    # print(f\"Number of iteration {index}\")\n",
    "    return result_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Triangle Iterative Triclass thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.810126582278481, 0.6938775510204082, 0.9444444444444444, 0.5483870967741935)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_values = []\n",
    "predictions = []\n",
    "\n",
    "# Prediction on every image\n",
    "for image in images:\n",
    "    result = triclass_threshold(image, threshold_triangle)\n",
    "\n",
    "    true_values.append(1 if image['Y'] > 0 else 0)\n",
    "    predictions.append(hf.classification_var(result, verbose=False, border=border))\n",
    "\n",
    "# Print result of classification, print metrics of classification\n",
    "print(predictions)\n",
    "accuracy_score(true_values, predictions), f1_score(true_values, predictions), precision_score(true_values, predictions), recall_score(true_values, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Otsu Iterative Triclass thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9113924050632911, 0.8852459016393444, 0.9, 0.8709677419354839)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_values = []\n",
    "predictions = []\n",
    "\n",
    "# Prediction on every image\n",
    "for image in images:\n",
    "    result = triclass_threshold(image, threshold_otsu)\n",
    "\n",
    "    true_values.append(1 if image['Y'] > 0 else 0)\n",
    "    predictions.append(hf.classification_var(result, verbose=False, border=border))\n",
    "\n",
    "# Print result of classification, print metrics of classification\n",
    "print(predictions)\n",
    "accuracy_score(true_values, predictions), f1_score(true_values, predictions), precision_score(true_values, predictions), recall_score(true_values, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Niblack thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.620253164556962, 0.16666666666666666, 0.6, 0.0967741935483871)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_values = []\n",
    "predictions = []\n",
    "\n",
    "# Prediction on every image\n",
    "for image in images:\n",
    "    result = image['img'] > threshold_niblack(image['img'], window_size=25, k=0.8)\n",
    "    \n",
    "    true_values.append(1 if image['Y'] > 0 else 0)\n",
    "    predictions.append(hf.classification_var(result, verbose=False, border=border))\n",
    "\n",
    "# Print result of classification, print metrics of classification\n",
    "print(predictions)\n",
    "accuracy_score(true_values, predictions), f1_score(true_values, predictions), precision_score(true_values, predictions), recall_score(true_values, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sauvola thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8607594936708861,\n",
       " 0.7999999999999999,\n",
       " 0.9166666666666666,\n",
       " 0.7096774193548387)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_values = []\n",
    "predictions = []\n",
    "\n",
    "# Prediction on every image\n",
    "for image in images:\n",
    "    result = image['img'] > threshold_sauvola(image['img'], window_size=25)\n",
    "    \n",
    "    true_values.append(1 if image['Y'] > 0 else 0)\n",
    "    predictions.append(hf.classification_var(result, verbose=False, border=border))\n",
    "\n",
    "# Print result of classification, print metrics of classification\n",
    "print(predictions)\n",
    "accuracy_score(true_values, predictions), f1_score(true_values, predictions), precision_score(true_values, predictions), recall_score(true_values, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning for Automatic IC Image Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:26:00.0, compute capability: 8.6\n",
      "\n",
      "Loaded (42) images from .\\data\\07022022_data\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow\n",
    "\n",
    "sess = tensorflow.compat.v1.Session(config=tensorflow.compat.v1.ConfigProto(log_device_placement=True))\n",
    "\n",
    "# # LOAD DATASETS\n",
    "images, labels = hf.load_tifdata(data_07022022, classes=2, label=True)\n",
    "# images, labels = hf.load_tifdata(data_04042022, classes=2, label=True, width=768, height=768)\n",
    "\n",
    "x, y = [], []\n",
    "for index, data in enumerate(images):\n",
    "    x.append(data['img'])\n",
    "    y.append(data['Y'])\n",
    "\n",
    "# # SPLIT INTO valid train\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=1)\n",
    "# x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.50, random_state=1) \n",
    "\n",
    "# ------------------------------------------\n",
    "# # Divide dataset into small images\n",
    "image_size = 128 # Size of images\n",
    "x_split, y_split = [], []\n",
    "for item_x, item_y in zip(x_train, y_train):\n",
    "    cropped_image = hf.crop_image(item_x, image_size, image_size)\n",
    "    x_split += cropped_image\n",
    "    y_split += [item_y for item in range(len(cropped_image))]\n",
    "x_train = np.array([item.reshape(item.shape[1], item.shape[0], 1) for item in x_split])\n",
    "y_train = np.array(y_split)\n",
    "\n",
    "x_split, y_split = [], []\n",
    "for item_x, item_y in zip(x_test, y_test):\n",
    "    cropped_image = hf.crop_image(item_x, image_size, image_size)\n",
    "    x_split += cropped_image\n",
    "    y_split += [item_y for item in range(len(cropped_image))]\n",
    "x_test = np.array([item.reshape(item.shape[1], item.shape[0], 1) for item in x_split])\n",
    "y_test = np.array(y_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "22/22 [==============================] - 8s 223ms/step - loss: 4.0193 - accuracy: 0.3887 - val_loss: 99.8373 - val_accuracy: 0.2222\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 4s 169ms/step - loss: 1.0898 - accuracy: 0.5407 - val_loss: 32.5642 - val_accuracy: 0.2222\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 4s 168ms/step - loss: 1.5270 - accuracy: 0.5573 - val_loss: 4.2399 - val_accuracy: 0.2222\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 4s 178ms/step - loss: 0.7193 - accuracy: 0.6804 - val_loss: 6.2690 - val_accuracy: 0.2222\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 4s 175ms/step - loss: 0.5823 - accuracy: 0.7296 - val_loss: 4.0220 - val_accuracy: 0.2222\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 4s 166ms/step - loss: 0.7333 - accuracy: 0.6785 - val_loss: 8.3007 - val_accuracy: 0.2222\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 4s 163ms/step - loss: 0.4881 - accuracy: 0.7481 - val_loss: 5.9844 - val_accuracy: 0.2222\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 4s 171ms/step - loss: 0.5135 - accuracy: 0.7287 - val_loss: 1.9896 - val_accuracy: 0.2222\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 4s 178ms/step - loss: 0.5151 - accuracy: 0.7472 - val_loss: 2.8323 - val_accuracy: 0.2222\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - 4s 170ms/step - loss: 0.5200 - accuracy: 0.7277 - val_loss: 1.9394 - val_accuracy: 0.2326\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - 4s 168ms/step - loss: 0.6436 - accuracy: 0.6908 - val_loss: 1.4817 - val_accuracy: 0.2361\n",
      "Epoch 12/20\n",
      "22/22 [==============================] - 4s 170ms/step - loss: 0.5752 - accuracy: 0.7405 - val_loss: 1.6515 - val_accuracy: 0.1701\n",
      "Epoch 13/20\n",
      "22/22 [==============================] - 4s 169ms/step - loss: 0.4780 - accuracy: 0.7438 - val_loss: 1.2335 - val_accuracy: 0.2413\n",
      "Epoch 14/20\n",
      "22/22 [==============================] - 4s 177ms/step - loss: 0.5802 - accuracy: 0.7410 - val_loss: 1.0695 - val_accuracy: 0.2500\n",
      "Epoch 15/20\n",
      "22/22 [==============================] - 4s 170ms/step - loss: 0.5336 - accuracy: 0.7202 - val_loss: 2.2556 - val_accuracy: 0.0330\n",
      "Epoch 16/20\n",
      "22/22 [==============================] - 4s 171ms/step - loss: 0.6491 - accuracy: 0.6856 - val_loss: 2.6145 - val_accuracy: 0.2222\n",
      "Epoch 17/20\n",
      "22/22 [==============================] - 4s 176ms/step - loss: 0.4823 - accuracy: 0.7642 - val_loss: 0.8864 - val_accuracy: 0.1997\n",
      "Epoch 18/20\n",
      "22/22 [==============================] - 4s 170ms/step - loss: 0.5011 - accuracy: 0.7254 - val_loss: 0.6764 - val_accuracy: 0.6319\n",
      "Epoch 19/20\n",
      "22/22 [==============================] - 4s 170ms/step - loss: 0.4799 - accuracy: 0.7566 - val_loss: 0.4337 - val_accuracy: 0.8993\n",
      "Epoch 20/20\n",
      "22/22 [==============================] - 4s 169ms/step - loss: 0.4591 - accuracy: 0.7585 - val_loss: 0.8643 - val_accuracy: 0.2222\n",
      "18/18 [==============================] - 1s 19ms/step - loss: 0.8643 - accuracy: 0.2222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8643131852149963, 0.2222222238779068]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 20\n",
    "num_classes = 2\n",
    "shape = x_train[0].shape\n",
    "# Architecture\n",
    "model = tf.keras.applications.ResNet50(\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_tensor=keras.layers.Input(shape=shape),\n",
    "    input_shape=shape,\n",
    "    pooling=max,\n",
    "    classes=num_classes,\n",
    "    classifier_activation=\"softmax\",\n",
    ")\n",
    "\n",
    "# Optimalizer\n",
    "optimalizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001, \n",
    "    beta_1=0.9, \n",
    "    beta_2=0.999, \n",
    "    epsilon=1e-07, \n",
    "    amsgrad=False, \n",
    "    name='Adam'\n",
    ")\n",
    "\n",
    "# Compile with Optimalizer, Metrics (accuracy), Categorical Crossentropy\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(), \n",
    "    optimizer=optimalizer,\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    to_categorical(y_train),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_data=(x_test, to_categorical(y_test)),\n",
    "    shuffle='batch',\n",
    "    use_multiprocessing=True, \n",
    "    workers=8\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "model.evaluate(x_test, to_categorical(y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get prediction for whole dataset including other class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded (79) images from .\\data\\04042022_data\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2911392405063291,\n",
       " 0.45098039215686275,\n",
       " 0.323943661971831,\n",
       " 0.7419354838709677)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = hf.load_tifdata(data_04042022, classes=2, label=True, width=768, height=768)\n",
    "# images, labels = hf.load_tifdata(data_07022022, classes=2, label=True)\n",
    "x, y = [], []\n",
    "for index, data in enumerate(images):\n",
    "    x.append(data['img'])\n",
    "    y.append(1 if data['Y'] else 0)\n",
    "\n",
    "# # Divide dataset into small images\n",
    "x_split, y_split = [], []\n",
    "for item_x, item_y in zip(x, y):\n",
    "    cropped_image = hf.crop_image(item_x, image_size, image_size)\n",
    "    x_split += cropped_image\n",
    "    y_split += [item_y for item in range(len(cropped_image))]\n",
    "cropped_len = len(cropped_image)\n",
    "x = np.array([item.reshape(item.shape[1], item.shape[0], 1) for item in x_split])\n",
    "y = np.array(y_split)\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "result = []\n",
    "predictions = np.argmax(model.predict(x), axis=1)\n",
    "for item in range(0, len(predictions)//cropped_len):\n",
    "    result.append(np.sum(predictions[item*cropped_len: (item+1)*cropped_len]))\n",
    "predictions = [1 if item > 0 else 0 for item in result]\n",
    "\n",
    "\n",
    "# Print result of classification, print metrics of classification\n",
    "print(predictions)\n",
    "accuracy_score(labels, predictions), f1_score(labels, predictions), precision_score(labels, predictions), recall_score(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded (100) images from .\\data\\07022022_data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.79, 0.8826815642458101, 0.79, 1.0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "\n",
    "# images, labels = hf.load_tifdata(data_04042022, classes=3, label=True, width=768, height=768)\n",
    "images, labels = hf.load_tifdata(data_07022022, classes=3, label=True)\n",
    "labels = [1 if item > 0 else 0 for item in labels]\n",
    "# images, labels = hf.load_tifdata(data_07022022, classes=2, label=True)\n",
    "accuracy_score(labels, predictions), f1_score(labels, predictions), precision_score(labels, predictions), recall_score(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Border for classification\n",
    "border = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classic k-means for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def km_clust(array, clusters, iteration=10):\n",
    "    \"\"\"\n",
    "    Wrap k-means from opencv\n",
    "    parameters \\n\n",
    "        array -> array to cluster \\n\n",
    "        clusters -> number of clusters \\n\n",
    "        iteration -> number of iteration for clustering \\n\n",
    "    return \\n\n",
    "        labels and centroids\n",
    "    \"\"\"\n",
    "    X = np.float32(array.reshape((-1, 1)))\n",
    "\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.85)\n",
    "\n",
    "    values, labels, centers = cv2.kmeans(X, clusters, None, criteria, iteration, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    return (centers, labels)\n",
    "    \n",
    "def kmeans_cls(img, clustes=2):\n",
    "    \"\"\"\n",
    "    Use k-means to clustering pixel in image\n",
    "    parameters \\n\n",
    "        img -> image to segmentent \\n\n",
    "        clusters -> number of clusters (default 2) \\n\n",
    "    return \\n\n",
    "        segmentated image\n",
    "    \"\"\"\n",
    "    values, labels = km_clust(img, clusters=clustes)\n",
    "\n",
    "    # Create the segmented array from labels and values\n",
    "    img_segm = np.choose(labels, values)\n",
    "\n",
    "    # Reshape the array as the original image\n",
    "    img_segm.shape = img.shape\n",
    "\n",
    "    result = np.where(img_segm == values[0], 1, 0)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8987341772151899,\n",
       " 0.8709677419354839,\n",
       " 0.8709677419354839,\n",
       " 0.8709677419354839)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "true_values = []\n",
    "\n",
    "# Prediction on every image\n",
    "for image in images:\n",
    "    result = kmeans_cls(image['img'], 2)\n",
    "    \n",
    "    true_values.append(1 if image['Y'] > 0 else 0)\n",
    "    predictions.append(hf.classification_var(result, border=border, verbose=False))\n",
    "\n",
    "# Print result of classification, print metrics of classification\n",
    "print(predictions)\n",
    "accuracy_score(true_values, predictions), f1_score(true_values, predictions), precision_score(true_values, predictions), recall_score(true_values, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classic k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matho\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:887: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8987341772151899,\n",
       " 0.8709677419354839,\n",
       " 0.8709677419354839,\n",
       " 0.8709677419354839)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "x, y = [], []\n",
    "# Load data into one array for clustering\n",
    "for index, data in enumerate(images):\n",
    "    x.append(np.array(data['img']).reshape(data['h'] * data['w']))\n",
    "    y.append(1 if data['Y'] > 0 else 0)\n",
    "\n",
    "X = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "# Get count of clusters\n",
    "total_clusters = len(np.unique(y))\n",
    "# Clustering from scikit-learn\n",
    "kmeans = MiniBatchKMeans(n_clusters = total_clusters)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Print result\n",
    "prediction = kmeans.labels_\n",
    "accuracy_score(true_values, predictions), f1_score(true_values, predictions), precision_score(true_values, predictions), recall_score(true_values, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptive k-means clustering algorithm for MR breast image segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization phase \n",
    "iteration = 10\n",
    "\n",
    "def fcir(areas, clusters):\n",
    "    \"\"\"\n",
    "    Compute fcir function\n",
    "    parameters \\n\n",
    "        areas -> pixels of image \\n\n",
    "        clusters -> value of cluster for pixel \\n\n",
    "    \"\"\"\n",
    "    return ((areas * 4 * math.pi) / np.power(clusters, 2))\n",
    "\n",
    "def goodness(areas, clusters):\n",
    "    \"\"\"\n",
    "    Compute goodness function\n",
    "    parameters \\n\n",
    "        areas -> pixels of image \\n\n",
    "        clusters -> value of cluster for pixel \\n\n",
    "    \"\"\"\n",
    "    sum_fcir = np.sum(fcir(areas, clusters) * areas)\n",
    "    sum_areas = np.sum(areas)\n",
    "    return sum_fcir/sum_areas\n",
    "\n",
    "def kmeans_(array, clusters=2, iteration=10, centers=cv2.KMEANS_RANDOM_CENTERS):\n",
    "    \"\"\"\n",
    "    Wrap k-means from opencv\n",
    "    parameters \\n\n",
    "        array -> array to cluster \\n\n",
    "        clusters -> number of clusters \\n\n",
    "        iteration -> number of iteration for clustering \\n\n",
    "        centers -> if centroid shoudl be random or predifined \\n\n",
    "    return \\n\n",
    "        labels and centroids\n",
    "    \"\"\"\n",
    "    X = np.float32(array.reshape((-1, 1)))\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.85)\n",
    "\n",
    "    if type(centers) is np.ndarray:\n",
    "        values, labels, centers = cv2.kmeans(X, clusters, None, criteria, iteration, cv2.KMEANS_USE_INITIAL_LABELS, centers)\n",
    "    else:\n",
    "        values, labels, centers = cv2.kmeans(X, clusters, None, criteria, iteration, centers)\n",
    "    return (centers, labels)\n",
    "\n",
    "def new_center_generation(centers):\n",
    "    \"\"\"\n",
    "    Create new centroid \n",
    "    parameters \\n\n",
    "        centers -> previous centers\n",
    "    return \\n\n",
    "        new centers\n",
    "    \"\"\"\n",
    "    # Define the brightest centroid\n",
    "    x = centers[1] if centers[1] > centers[0] else centers[0]\n",
    "    y = centers[0] if centers[1] > centers[0] else centers[1]\n",
    "    # Compute distance from old\n",
    "    distance = x - y\n",
    "\n",
    "    # Get random value\n",
    "    random_value = np.random.rand(1)[0]\n",
    "\n",
    "    # Compute new centroid\n",
    "    z = x + distance * random_value\n",
    "    w = x - distance * random_value\n",
    "    return np.array([z, w])\n",
    "\n",
    "def adaptive_kmeans(img, clusters=2, iteration=10, itstop=5):\n",
    "    \"\"\"\n",
    "    Adaptive k-means for pixel segmentation\n",
    "    parameters \\n\n",
    "        img -> image for segmentation \\n\n",
    "        clusters -> number of clusters \\n\n",
    "        iteration -> number of iteration for clustering \\n\n",
    "        itstop -> number of iteration for whole process\n",
    "    return \\n\n",
    "        segmentated image\n",
    "    \"\"\"\n",
    "    # First get starting centroid\n",
    "    values, labels = kmeans_(img, clusters=clusters, iteration=iteration)\n",
    "\n",
    "    img_re = img.reshape((img.shape[0] * img.shape[1], 1))\n",
    "    img_segm = np.choose(labels, values)\n",
    "    \n",
    "    # Get starting goodness \n",
    "    goodness_prev = goodness(img_re, img_segm)\n",
    "    clusters_prev = values\n",
    "\n",
    "    # Iterative use k-means\n",
    "    for index in range(itstop):\n",
    "        # K-means\n",
    "        values, labels = kmeans_(img, clusters=clusters, iteration=iteration, centers=clusters_prev)\n",
    "        img_segm = np.choose(labels, values)\n",
    "\n",
    "        # Compute goodness \n",
    "        goodness_val = goodness(img_re, img_segm)\n",
    "        # Choose new centroids\n",
    "        if goodness_val <= goodness_prev:\n",
    "            goodness_prev = goodness_val\n",
    "            clusters_prev = values\n",
    "        else:\n",
    "            values = new_center_generation(values)\n",
    "\n",
    "    img_segm.shape = img.shape\n",
    "    result = np.where(img_segm == values[0], 0, 1)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 1 ... 1 1 0]\n",
      " ...\n",
      " [0 1 1 ... 1 0 1]\n",
      " [1 1 1 ... 1 1 0]\n",
      " [0 1 0 ... 1 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8734177215189873,\n",
       " 0.8148148148148149,\n",
       " 0.9565217391304348,\n",
       " 0.7096774193548387)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "true_values = []\n",
    "\n",
    "# Prediction on every image\n",
    "for image in images:\n",
    "    result = adaptive_kmeans(image['img'], 2)\n",
    "\n",
    "    true_values.append(1 if image['Y'] > 0 else 0)\n",
    "    predictions.append(hf.classification_var(result, border=border, verbose=False))\n",
    "\n",
    "# Print result of classification, print metrics of classification\n",
    "print(result)\n",
    "accuracy_score(true_values, predictions), f1_score(true_values, predictions), precision_score(true_values, predictions), recall_score(true_values, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "011e8061a232e047af60ab8a1e16de34c13c882dfab248521bac64cc4d5c32dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
